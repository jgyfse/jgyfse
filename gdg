seen_urls = set()
 
# 在爬取过程中
for url in urls_to_crawl:
    if url not in seen_urls:
        # 爬取数据的操作
        seen_urls.add(url)g
import requests
from requests.exceptions import RequestException
 
MAX_RETRIES = 3
 
def fetch_data(url):
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response.text
        except RequestException as e:
            print(f"Request failed: {e}")
            retries += 1
            time.sleep(2)  # 等待一段时间后重试
    return None
